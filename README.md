# DataScience-Reference-Repository
View all valued references that I have came across and have used to enhance my knowledge and understanding of various DataScience Cocenpts

* [Getting Started with  Machine Learning](#get-started-with-machine-learning)
* [Popular Algorithms in  Machine Learning](#popular-algorithms-in-machine-learning)
* [Introduction to Deep Learning Algorithms](#introduction-to-deep-learning-algorithms)
* [Deep Learning Research Papers](#deep-learning-research-papers)






## Get started with Machine Learning

Datascience is made up of 3 modules 
* 1)Artificial Intelligence 
* 2)Machine Learning
* 3)Deep Learning . 

AI being the bigger umbrella incorporates everything that a machine (i.e. computers) do to mimic human like behavior.
Machine Learning is a subfield of Artificial Intelligence .Machine Learning becoming a buzz word in the IT industry, it's almost impossible to think of a industry that doesnt need intelligent solutions by adapting to Machine Learning paradigm. 
Here I put forward my own experience , the direction in which I preferred learning the subject in a way to make it more easy and convenient  for anyone who is interested in knowing "How Machines Learn ?? "

1) Lets get started with standard  Wikipedia definition of Machine learning, ML is a *field of computer science that gives computers the ability to learn without being explicitly programmed*. 
To get understanding of how Machines Learn and how Machine Learning paradigm is different from regular programming paradigm , I would recommend the audience to go through the below references . 

	*  [A Friendly Intro to Machine Learning](https://www.youtube.com/watch?v=IpGxLWOIZy4): Introductory video helping you understand Machine Learning
	* [Machine Learning Introduction](https://www.youtube.com/watch?v=seG9J49bBYI): A video that gets you introduced with ML terminologies and concise appropriate definitions 
	* [What is Machine Learning?](https://www.youtube.com/watch?v=WXHM_i-fgGo): This video introduces you to prominent different ways in which we can make Machines learn , viz. Supervised Learning, UnSupervised Learning , Reinforcement Learning (Adaptive Learning)
	* [Basic Machine Learning Algorithms Overview](https://www.youtube.com/watch?v=ggIk08PNcBo): This video gives insights to different algorithms used for different use cases in Machine Learning
	* [Machine Learning from Zero to Hero](https://medium.freecodecamp.org/machine-learning-how-to-go-from-zero-to-hero-40e26f8aa6da): This video is quite interesting in gaining interest in the subject of Machine Learning. 
	
	
## Popular Algorithms in Machine Learning

1) **Linear Regression**: 
	* [Coursera Course - Linear Regression with One Variable](https://www.youtube.com/watch?v=kHwlB_j7Hkc)
	* [Simple Linear Regression](https://www.youtube.com/watch?v=ZkjP5RJLQF4)
	* [Linear Regression for Machine Learning](https://machinelearningmastery.com/linear-regression-for-machine-learning/)

2) **Decision Trees**:
	* [Nando de Freitas Lecture](https://www.youtube.com/watch?v=-dCtJjlEEgM)
	* [Decision Trees in ML](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052)
	
3)	**Random Forest**:
	* [Nando de Freitas Lecture](https://www.youtube.com/watch?v=3kYujfDgmNk)
	* [Random Forest in ML](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)
	
4) **Logistic Regression**: 
	* [Coursera Course - Logistic Regression and Classification](https://www.youtube.com/watch?v=-la3q9d7AKQ)
	* [Logistic Regression - An Introduction](https://www.youtube.com/watch?v=zAULhNrnuL4)
	* [Stanford Logistic Regression Overview](http://ufldl.stanford.edu/tutorial/supervised/LogisticRegression/)
	* [Siraj Raval Logistic Regression Tutorial](https://www.youtube.com/watch?v=D8alok2P468)

5) **K Nearest Neighbors**: 
	* [K Nearest Neighbors](https://www.youtube.com/watch?v=k_7gMp5wh5A)
	* [GeeksForGeeks Explanation](https://www.geeksforgeeks.org/k-nearest-neighbours/)
	* [Udacity Explanation of KNNs](https://www.youtube.com/watch?v=mpU84OJ5vdQ)

6)  **Support Vector Machine**: 
	* [Support Vector Machine by Instructor Patrick Winston](https://www.youtube.com/watch?v=_PwhiWxHK8o)
	* [Analytics Vidhya Explanation](https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)
	* [SVM Explanation on scikit learn ](https://scikit-learn.org/stable/modules/svm.html)

		
7) **K-Means**: 
	* [Coursera Course - K Means](https://www.youtube.com/watch?v=hDmNF9JG3lo)
	* [Stanford K Means Overview](http://stanford.edu/~cpiech/cs221/handouts/kmeans.html)
	* [Siraj Raval K Means Tutorial](https://www.youtube.com/watch?v=9991JlKnFmk)
	
## Introduction to Deep Learning Algorithms	

1) **Neural Networks**: Getting started with Deep learning, Neural Networks Deep Learning is subfield of Machine Learning an advance take on Machine Learning . Learning here happens through layers stacked one after the other constituting to a Neural Network architecture. The whole idea revolves around designing the best Neural Network architecture that leads to highest accuracy and lowest error.
	* [A Friendly Introduction to Deep Learning and Neural Networks](https://www.youtube.com/watch?v=BR9h47Jtqyw)
	* [3Blue1Brown Neural Network Playlist](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
	* [How Deep Neural Networks Work](https://www.youtube.com/watch?v=ILsA4nyG7I0)
	* [Neural Network Playground](http://playground.tensorflow.org)
	* [Michael Nielsen Book on NNs](http://neuralnetworksanddeeplearning.com/chap1.html)
	

2) **Artificial Neural Networks**:  Artificial Neural Network(CNN) take inspiration from a section in the human brain named temporal Lobe (responsible for storing all your memories for long), ANN learn inherent Latent patterns in data and memorize them during training to recall them later to gice predictions for upcoming data
	* [Neural Nets](https://www.youtube.com/watch?v=uXt8qF2Zzfo): in this video, Prof. Winston introduces neural nets and back propagation.
	* [How Deep Neural Networks Work:Brandon Rohrer](https://www.youtube.com/watch?v=ILsA4nyG7I0)
	* [Gradient Descent:Kilian Weinberger ](https://www.youtube.com/watch?v=o6FfdP2uYh4)
	
	
3) **Convolutional Neural Networks**:  Convolutional Neural Network(CNN) take inspiration from a section in the human brain named Ocipital Lobe (responsible for vision), CNN hence is very popularly used in image processing i.e. unstructured data processing.
	* [CS231n Visual Recognition](http://cs231n.github.io/convolutional-networks/)
	* [CS231n Video Lectures](https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC)
	* [Brandon Rohrer YouTube Tutorial](https://www.youtube.com/watch?v=FmpDIaiMIeA)
	* [Andrew Ng's CNN Course](https://www.youtube.com/playlist?list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud)

4) **Recurrent Neural Networks**: Recurrent Neural Network are Neural Network that are recursive by nature .
	* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
	* [CS 224D Video Lectures](https://www.youtube.com/playlist?list=PLCJlDcMjVoEdtem5GaohTC1o9HTTFtK7_)
	* [RNNs and LSTMs](https://www.youtube.com/watch?v=WCUNPb-5EYI)
	* [Recurrent Neural Networks - Intel Nervana](https://www.youtube.com/watch?v=Ukgii7Yd_cU)
	* [Introduction to RNNs](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)

5) **Reinforcement Learning**: Reinforcement Learning is a subfield of Machine Learning, but is also a general purpose formalism for automated decision-making and AI.
	* [David Silver's Reinforcement Learning Course](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa)
	* [Simple Reinforcement Learning with Tensorflow](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)


## Deep Learning Research Papers:Papers that were quite influential in building and improving Deep Learning paradigm 

* [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (2017), Andrew G. Howard et al.](https://arxiv.org/pdf/1704.04861.pdf)
* [A Knowledge-Grounded Neural Conversation Model (2017), Marjan Ghazvininejad et al.](https://arxiv.org/pdf/1702.01932.pdf)
* [Evolution Strategies as a Scalable Alternative to Reinforcement Learning (2017), T. Salimans et al.](https://arxiv.org/pdf/1703.03864v1.pdf)
* [Learning to discover cross-domain relations with generative adversarial networks (2017), T. Kim et al.](https://arxiv.org/pdf/1703.05192v1.pdf)
* [PixelNet: Representation of the pixels, by the pixels, and for the pixels (2017), A. Bansal et al.](https://arxiv.org/pdf/1702.06506v1.pdf)
* [Understanding deep learning requires rethinking generalization (2017), C. Zhang et al.](https://arxiv.org/pdf/1611.03530.pdf)
* [Recurrent neural network based language model (2010), T. Mikolov et al.](http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf)
* [Understanding the difficulty of training deep feedforward neural networks (2010), X. Glorot and Y. Bengio](https://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf)
* [Gradient-based learning applied to document recognition (1998), Y. LeCun et al.](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)
* [Deep voice: Real-time neural text-to-speech (2017), S. Arik et al.](https://arxiv.org/pdf/1702.07825v2.pdf)
* [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)
* [GoogLeNet](https://arxiv.org/pdf/1409.4842v1.pdf)
* [VGGNet](https://arxiv.org/pdf/1409.1556v6.pdf)
* [ZFNet](https://arxiv.org/pdf/1311.2901v3.pdf)
* [ResNet](https://arxiv.org/pdf/1512.03385.pdf)
* [R-CNN](https://arxiv.org/pdf/1311.2524v5.pdf)
* [Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf)
* [Adversarial Images](https://arxiv.org/pdf/1412.1897.pdf)
* [Generative Adversarial Networks](https://arxiv.org/pdf/1406.2661v1.pdf)
* [Spatial Transformer Networks](https://arxiv.org/pdf/1506.02025.pdf)
* [DCGAN](https://arxiv.org/pdf/1511.06434v2.pdf)
* [Synthetic Gradients](https://arxiv.org/pdf/1608.05343v1.pdf)
* [Memory Networks](https://arxiv.org/pdf/1410.3916v11.pdf)
* [Mixture of Experts](https://arxiv.org/pdf/1701.06538.pdf)
* [Neural Turing Machines](https://arxiv.org/pdf/1410.5401.pdf)
* [Alpha Go](https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf)
* [Atari DQN](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)
* [Word2Vec](https://arxiv.org/pdf/1310.4546.pdf)
* [GloVe](https://nlp.stanford.edu/pubs/glove.pdf)
* [A3C](https://arxiv.org/pdf/1602.01783v2.pdf)
* [Gradient Descent by Gradient Descent](https://arxiv.org/pdf/1606.04474v1.pdf)
* [Rethinking Generalization](https://arxiv.org/pdf/1611.03530v1.pdf)
* [Densely Connected CNNs](https://arxiv.org/pdf/1608.06993v1.pdf)
* [EBGAN](https://arxiv.org/pdf/1609.03126v1.pdf)
* [Wasserstein GAN](https://arxiv.org/pdf/1701.07875.pdf)
* [Style Transfer](https://arxiv.org/pdf/1603.08155v1.pdf)
* [Pixel RNN](https://arxiv.org/pdf/1601.06759v2.pdf)
* [Dynamic Coattention Networks](https://arxiv.org/pdf/1611.01604v2.pdf)
* [Convolutional Seq2Seq Learning](https://arxiv.org/pdf/1705.03122.pdf)
* [Seq2Seq](https://arxiv.org/pdf/1409.3215.pdf)
* [Dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
* [Batch Norm](https://arxiv.org/pdf/1502.03167.pdf)
* [Large Batch Training](https://arxiv.org/pdf/1609.04836.pdf)
* [Transfer Learning](http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf)
* [Adam](https://arxiv.org/pdf/1412.6980.pdf)
* [Speech Recognition](https://arxiv.org/pdf/1303.5778.pdf)
* [Relational Networks](https://arxiv.org/pdf/1706.01427.pdf)
* [Influence Functions](https://arxiv.org/pdf/1703.04730.pdf)
* [ReLu](https://arxiv.org/pdf/1611.01491.pdf)
* [Xavier Initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
* [Saddle Points and Non-convexity of Neural Networks](https://arxiv.org/pdf/1406.2572.pdf)
* [Overcoming Catastrophic Forgetting in NNs](https://arxiv.org/pdf/1612.00796.pdf)
* [Quasi-Recurrent Neural Networks](https://arxiv.org/pdf/1611.01576.pdf)
* [Escaping Saddle Points Efficiently](https://arxiv.org/pdf/1703.00887.pdf)
* [Progressive Growing of GANs](http://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of//karras2017gan-paper.pdf)
* [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
* [Dynamic Routing Between Capsules](https://arxiv.org/pdf/1710.09829.pdf)
* [Unsupervised Machine Translation with Monolingual Corpora](https://arxiv.org/pdf/1711.00043.pdf)
* [Population Based Training of NN's](https://arxiv.org/pdf/1711.09846.pdf)
* [Learned Index Structures](https://arxiv.org/pdf/1712.01208v1.pdf)
* [Visualizing Loss Landscapes](https://arxiv.org/pdf/1712.09913v1.pdf)
* [DenseNet](https://arxiv.org/pdf/1608.06993.pdf)
* [SqueezeNet](https://arxiv.org/pdf/1602.07360.pdf)
* [WaveNet](https://arxiv.org/pdf/1609.03499.pdf)
* [Hidden Technical Debt in ML Systems](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf)
* [MobileNets](https://arxiv.org/pdf/1704.04861.pdf)
* [Learning from Imbalanced Data](http://www.ele.uri.edu/faculty/he/PDFfiles/ImbalancedLearning.pdf)
